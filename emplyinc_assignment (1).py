# -*- coding: utf-8 -*-
"""emplyInc_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZqtKomedMrW4EtCNIoyALeQXVxtEJkWo
"""

pip install transformers

#importing the libraries
import requests
from bs4 import BeautifulSoup
from transformers import BartForConditionalGeneration, BartTokenizer

#web scraping
def scrape_link(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extract headings and content
    headings = [heading.text for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]
    content = [p.text for p in soup.find_all('p')]

    return headings,content

wikipedia_url = 'https://en.wikipedia.org/wiki/Alexander_the_Great'
headings, content = scrape_link(wikipedia_url)
print("Headings:", headings)
print("Content:", content)

#summarzation of the  contennt

def summarize_bart(text):
    tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
    model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

    # Limiting the input text to the first 512 tokens
    inputs = tokenizer.encode("summarize: " + text[:512], return_tensors="pt", max_length=1024, truncation=True)

    # Adjust parameters for faster processing
    summary_ids = model.generate(inputs, max_length=100, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)

    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def summarize_sections(headings, content):
    summaries = {heading: summarize_bart(" ".join(content[i:i+3])) for i, heading in enumerate(headings)}
    return summaries

# Print the summaries
headings, content = scrape_link('https://en.wikipedia.org/wiki/Alexander_the_Great')
summaries = summarize_sections(headings, content)
print("Summaries:", summaries)

def generate_html_page(summaries):
    html_content = f"<html><head><title>Summarized Wikipedia Page</title></head><body>"
    html_content += "".join([f"<h2>{heading}</h2><p>{summary}</p>" for heading, summary in summaries.items()])
    html_content += "</body></html>"

    with open("summarized_page.html", "w", encoding="utf-8") as html_file:
        html_file.write(html_content)
generate_html_page(summaries)
print("HTML page generated.")

with open("summarized_page.html", "r", encoding="utf-8") as html_file:
    html_content = html_file.read()

print(html_content)

from google.colab import files

# Download the HTML file
files.download("summarized_page.html")

import nltk

nltk.download('stopwords')



pip install readability-lxml

pip install textstat

print(content[1])

for value in summaries.values():
  print(value)

from nltk import word_tokenize
import textstat

def readability_score(text):
    # Use a simple readability metric (Flesch-Kincaid Grade Level)
    return textstat.flesch_kincaid_grade(text)

def coherence_score(current_summary, previous_summary):

    return 0.8  # Replace with your coherence calculation

def evaluate_summary(original_text,summary,previous_summary):
    readability = readability_score(summary)
    coherence = coherence_score(summary, previous_summary)
    return {"Readability": readability, "Coherence": coherence}

# Example Usage:
original_content = content[1]
previous_summary = None  # No previous summary for the first heading

evaluations = {}

for heading, summary in summaries.items():
    metrics = evaluate_summary(original_content,summary,previous_summary)
    evaluations[heading] = metrics
    previous_summary = summary

# Print or use the evaluations as needed
for heading, metrics in evaluations.items():
    print(f"{heading} Metrics:")
    print(f"Readability Score: {metrics['Readability']}")
    print(f"Coherence Score: {metrics['Coherence']}\n")

!pip install rouge

import torch
from rouge import Rouge

generated_summary =content[1]
reference_summary = value
rouge = Rouge()

scores = rouge.get_scores(generated_summary, reference_summary)
# Print the results
print(scores)



import torch
from torchtext.data.metrics import bleu_score


# Split the reference summary into a list of tokens
reference_tokens = reference_summary.split()

# Split the generated summary into a list of tokens
generated_tokens = generated_summary.split()

# calculate the BLEU score
score = bleu_score([generated_tokens], [[reference_tokens]])
print(f'BLEU Score: {score*100:.2f}')





print(headings)

def scrape_link(url):
    wikipedia_url = 'https://en.wikipedia.org/wiki/Alexander_the_Great'
    headings, content = scrape_link(wikipedia_url)
    return headings, content

print(content[1])

# Print headings and content
print("Headings:", headings)
print("Content:", content)

